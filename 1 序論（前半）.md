### 0.序論
#### 教師なし学習
* クラスタリング
* 密度推定: 入力空間におけるデータの分布を求める
* 視覚化: 高次元のデータを2~3次元に射影

#### 密度推定
有限の標本点から（各標本点はなんらかの関数に従い生成されていると仮定し、その隠されたパラメータを推定することで）標本点の存在しない全体の分布を推定すること

### 1.1 多項式曲線フィッティング
* データは学習しようとする規則性を保持しているが、観測値には何らかのランダムノイズが混在している
* 多項式曲線フィッティングでは次数を増やす（=モデルの表現力を高める）と過学習が発生する
* 過学習は自由度の高い多項式（=表現力の高いモデル）がランダムノイズに引きずられている状態
* 過学習の対策としてはデータ数を増やす（=ランダムノイズの影響が低減される） or 正則化 or ベイズ的アプローチ
* 正則化する場合には正則化項の最適なパラメータを検証する必要がある
* 多項式フィッティングは*x*に関しては非線形だが*w*に関しては線形（=つまり線形回帰のアプローチ）

#### 正則化
* 1次の正則化（Lasso回帰）: (余分なパラメータが0となるため)次元圧縮
* 2次の正則化（Ridge回帰）: Lasso回帰よりは精度が高い

### 本章の用語
以下の用語は意味を把握すること
* 周辺確率
* 確率密度
* 累積分布関数
* 確率質量関数
* 超パラメータ：モデルパラメータ(測定対象)の分布を制御するパラメータ

### 1.2.3 ベイズ確率
* どちらも与えられた観測データ*D*を元にパラメータベクトル*w*の推定を行う
* 事後確率 ∝ 尤度関数 x 事前確率
* 尤度関数は*w*を固定した場合における*D*の発生しやすさ
* 尤度関数は*w*の確率密度ではないので積分して1にはならない

#### 頻度主義の前提
* 頻度主義では*w*は固定値（真の*w*の値が一意に存在する）として扱う
* *D*はノイズを含む分布として扱う
* *D*から*w*を推定するが、*w*に関する不確実性は*D*の分布が原因の誤差
* *D*が増えれば*w*の推定結果は真の値に近づく(=誤差範囲が狭まる)

#### 頻度主義のアプローチ
* 最尤推定(=尤度の最大化)により*w*を求める
* これは与えられた*D*の確率を最大化する*w*を選ぶことを意味する
* 機械学習では尤度関数の対数を（-1）倍したものを誤差関数として扱う
* 尤度関数の最大化は誤差関数の最小化と同じ

#### ベイズ主義の前提
* ベイズ主義では*w*は確率分布（*w*は分布として与えられるので一意に定まらない）として扱う
* *D*は分布ではなく実際に観測結果を真の値として扱う

#### ベイズ推定のアプローチ
* *D*を観測した事後に*w*に関する不確実性を事後分布として更新する
* ベイズ推定では最尤推定を行わない?

#### 頻度主義とベイズ主義の差分の例
日本人の成人男性の平均身長を考える場合、どちらも推定結果には不確実性があるがその扱いが異なる
* 頻度主義: 区間XXXからYYYの間に平均身長の真の値が存在すると想定する
* ベイズ主義: 平均身長の確率分布を想定する（=平均身長は固定の値ではなく変数）

### 1.2.4 ガウス分布
* **頻度主義の計算**
* ガウス分布のiidを仮定して、*D*から最尤推定を利用してガウス分布の*μ*および*σ*を推定するとサンプル平均及びサンプル分散と一致する
* サンプル平均の期待値は真の*μ*と一致するが、サンプル分散の期待値は真の*σ*よりも過小評価される

### 1.2.5 曲線フィッティング再訪
* **頻度主義の計算**
* ガウス分布の平均値が*w*で定まると仮定して、最尤推定から*w*および*β*を推定した
* *D*に存在しない未知の入力に対しても、その出力を分布という形で予測できる
* 最小2乗法はノイズがガウス分布に従うと仮定した場合の尤度関数の最大化と同じ
* **ベイズ主義の計算**
* *w*に関する事後分布を最大化（=最大事後推定 or MAP推定）した場合に正則化が導出される（最優関数は頻度主義と同じ）
