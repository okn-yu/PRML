## 0.序論
### 教師なし学習
* クラスタリング
* 密度推定: 入力空間におけるデータの分布を求める
* 視覚化: 高次元のデータを2~3次元に射影

#### 密度推定
有限の標本点から（各標本点はなんらかの関数に従い生成されていると仮定し、その隠されたパラメータを推定することで）標本点の存在しない全体の分布を推定すること

### 1.1 多項式曲線フィッティング
* データは学習しようとする規則性を保持しているが、観測値には何らかのランダムノイズが混在している
* 多項式曲線フィッティングでは次数を増やす（=モデルの表現力を高める）と過学習が発生する
* 過学習は自由度の高い多項式（=表現力の高いモデル）がランダムノイズに引きずられている状態
* 過学習の対策としてはデータ数を増やす（=ランダムノイズの影響が低減される） or 正則化 or ベイズ的アプローチ
* 正則化する場合には正則化項の最適なパラメータを検証する必要がある

#### 正則化
* 1次の正則化（Lasso回帰）: (余分なパラメータが0となるため)次元圧縮
* 2次の正則化（Ridge回帰）: Lasso回帰よりは精度が高い

### 本章の用語
以下の用語は意味を把握すること
* 周辺確率
* 確率密度
* 累積分布関数
* 確率質量関数

### 1.2.3 ベイズ確率
* どちらも与えられた観測データ*D*を元にパラメータベクトル*w*の推定を行う
* 事後確率 ∝ 尤度関数 x 事前確率
* 尤度関数は*w*を固定した場合における*D*の発生しやすさ
* 尤度関数は*w*の確率密度ではないので積分して1にはならない

#### 頻度主義の前提
* 頻度主義では*w*は固定値（真の*w*の値が一意に存在する）として扱う
* *D*はノイズを含む分布として扱う
* *D*から*w*を推定するが、*w*に関する不確実性は*D*の分布が原因の誤差
* *D*が増えれば*w*の推定結果は真の値に近づく(=誤差範囲が狭まる)

#### 頻度主義のアプローチ
* 最尤推定(=尤度の最大化)により*w*を求める
* これは与えられた*D*の確率を最大化する*w*を選ぶことを意味する
* 機械学習では尤度関数の対数を（-1）倍したものを誤差関数として扱う
* 尤度関数の最大化は誤差関数の最小化と同じ

#### ベイズ主義の前提
* ベイズ主義では*w*は確率分布（*w*は分布として与えられるので一意に定まらない）として扱う
* *D*は分布ではなく実際に観測結果を真の値として扱う

#### ベイズ推定のアプローチ
* *D*を観測した事後に*w*に関する不確実性を事後分布として更新する
* ベイズ推定では最尤推定を行わない?

### 1.2.4 ガウス分布
* *頻度主義の計算*
* ガウス分布のiidを仮定して、*D*から最尤推定を利用してガウス分布の*μ*および*σ*を推定した

### 1.2.5 曲線フィッティング再訪
* *頻度主義の計算*
* ガウス分布の平均値が*w*で定まると仮定して、最尤推定から*w*および*β*を推定した
* *D*に存在しない未知の入力に対しても、その出力を分布という形で予測できる
* 最小2乗法はノイズがガウス分布に従うと仮定した場合の尤度関数の最大化と同じ
* *ベイズ主義の計算*
* *w*に関する事前分布を仮定した場合に正則化が導出される




